{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfXDFEokZ_0Z"
   },
   "source": [
    "## **Part 1: Optimization (80 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bLbMLuRhT75"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NaKKxXqZztL"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Additional Python files\n",
    "from LogisticRegression import LogisticRegression, TrainingResults\n",
    "from Gradient import *\n",
    "LogisticRegression.gradient = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Fog75jiEaGPs",
    "outputId": "8dffb6b0-95cd-43fa-c2da-b35f8a3687f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define datasets paths\n",
    "diabetes_train_dir = r'diabetes/diabetes_train.csv'\n",
    "diabetes_val_dir = r'diabetes/diabetes_val.csv'\n",
    "diabetes_test_dir = r'diabetes/diabetes_test.csv'\n",
    "\n",
    "diabetes_train_df = pd.read_csv(diabetes_train_dir, engine=\"python\", error_bad_lines=False)\n",
    "diabetes_val_df = pd.read_csv(diabetes_val_dir, engine=\"python\", error_bad_lines=False)\n",
    "diabetes_test_df = pd.read_csv(diabetes_test_dir, engine=\"python\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-Target split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into feature and target\n",
    "diabetes_train_X =  diabetes_train_df.drop('Outcome', axis=1)\n",
    "diabetes_train_y = diabetes_train_df.loc[:, 'Outcome']\n",
    "diabetes_val_X = diabetes_val_df.drop('Outcome', axis=1)\n",
    "diabetes_val_y = diabetes_val_df.loc[:, 'Outcome']\n",
    "diabetes_test_X = diabetes_test_df.drop('Outcome', axis=1)\n",
    "diabetes_test_y = diabetes_test_df.loc[:, 'Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH-u4_iUeP4H"
   },
   "source": [
    "## 1. Gradient descent\n",
    "You should first start by running the logistic regression code using the given implementation. This will serve as a baseline for the following steps. Find a learning rate and a number of training iterations such that the model has fully converged to a solution. Make sure to provide empirical evidence supporting your decision (e.g. training and validation accuracy as a function of number of training iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "QsM0_8_Jei2R",
    "outputId": "0d63bdd5-8de2-4d43-daef-004137fc62dd"
   },
   "outputs": [],
   "source": [
    "# method to create and fit the LR model\n",
    "def get_acc_list(lr, max_iters, itv):\n",
    "    model = LogisticRegression(verbose=True, learning_rate=lr, max_iters=max_iters)\n",
    "    acc_list = model.fit_for_vis(train_X, train_y, val_X, val_y, itv)\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I94i7HUfPRq"
   },
   "source": [
    "We will fit the model with the following learning rates: [0.2, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "max_iter = 1e6\n",
    "itv = int(1e3)\n",
    "lr_list = [0.2, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallerize training\n",
    "accs_list = Parallel(n_jobs=-1, verbose=10)(delayed(get_acc_list)(i, max_iter, itv) for i in lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "length = len(accs_list[0])\n",
    "row = list(range(1, length*itv, itv))\n",
    "[plt.plot(row, accs_list[i]) for i in range(len(accs_list))]\n",
    "plt.legend(['0.2', '1e-1', '1e-2', '1e-3', '1e-4', '1e-5', '1e-6', '1e-7'],\n",
    "           bbox_to_anchor=(1.04,1))\n",
    "plt.grid()\n",
    "plt.title(\"Change in validation accuracy over iterations\")\n",
    "# plt.savefig('/content/drive/MyDrive/COMP551/mini2/figures/lrs_compare_max_itrs=1e6.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, increase the max_iters and try again with [1e-4, 1e-5, 1e-6, 1e-7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "new_max_iter = 3*(1e6)\n",
    "new_itv = int(1e4)\n",
    "new_lr_list = [1e-4, 1e-5, 1e-6, 1e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accs_list = []\n",
    "for i in range(len(new_lr_list)):\n",
    "    result = get_acc_list(new_lr_list[i], new_max_iter, new_itv)\n",
    "    print('\\n')\n",
    "    new_accs_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "new_length = len(new_accs_list[0])\n",
    "new_row = list(range(0, new_length*new_itv, new_itv))\n",
    "[plt.plot(new_row, new_accs_list[i]) for i in range(len(new_accs_list))]\n",
    "plt.legend(['1e-4', '1e-5', '1e-6', '1e-7'],\n",
    "           bbox_to_anchor=(1.04,1))\n",
    "plt.grid()\n",
    "plt.title(\"Change in validation accuracy over iterations\")\n",
    "plt.savefig('/content/drive/MyDrive/COMP551/mini2/figures/lrs_compare_max_itrs=3_1e6.png',\n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Configuration\n",
    "From the plot above:\n",
    "\n",
    "max_iters = 1.8e6\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "---------------------------\n",
    "epsilon=1e-4 (default value) but the norm of the gradient didn't decrease below 1e-4 in my experiments, so this parameter is not tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkEHH6eRhAAh"
   },
   "source": [
    "## 2. Mini-batch stochastic gradient descent\n",
    "Implement mini-batch stochastic gradient descent. Then, using growing minibatch sizes (e.g. 8, 16, 32, ...) com- pare the convergence speed and the quality of the final solution to the fully batched baseline. What configuration works the best among the ones you tried ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9poLNA32hELH"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "# function to create and fit LR model and receive training info\n",
    "def get_training_results_batch(lr, max_iters, itv, batch_size, max_epochs, momentum):\n",
    "    model = LogisticRegression(verbose=True, learning_rate=lr, max_iters=max_iters)\n",
    "    return model.fit_for_vis_complex(diabetes_train_X, diabetes_train_y, diabetes_val_X, diabetes_val_y, itv, batch_size, max_epochs, momentum)\n",
    "\n",
    "# helper function to have all time lists be the same length\n",
    "def same_length_lsts(results):\n",
    "    max_time_num_itv = 0\n",
    "    for r in results:\n",
    "        if len(r.acc_list_time) > max_time_num_itv:\n",
    "            max_time_num_itv = len(r.acc_list_time)\n",
    "    for r in results:\n",
    "        if len(r.acc_list_time) < max_time_num_itv:\n",
    "            r.acc_list_time += [r.acc_list_time[-1]] * (max_time_num_itv - len(r.acc_list_time))\n",
    "        if len(r.grad_list_time) < max_time_num_itv:\n",
    "            r.grad_list_time += [r.grad_list_time[-1]] * (max_time_num_itv - len(r.grad_list_time))\n",
    "    return results\n",
    "\n",
    "# method to plot the epochs results\n",
    "def plot_epochs(results, num_epochs):\n",
    "    epoch_row = list(range(1, num_epochs + 1))\n",
    "    [plt.plot(epoch_row, results[i].acc_list_epoch) for i in range(len(results))]\n",
    "    plt.legend(batch_sizes, bbox_to_anchor=(1.04, 1))\n",
    "    plt.grid()\n",
    "    plt.title(f\"Change in validation accuracy over iterations by batch size (lr = {learning_rate})\")\n",
    "    plt.savefig('./' + num_epochs + '_acc_epoch_batch')\n",
    "    \n",
    "# method to plot all other results\n",
    "def plot_results(results, lr_val):\n",
    "    length = len(results[0].acc_list_it)\n",
    "    it_row = list(range(1, length*itv, itv))\n",
    "    time_row = list(range(0, 15*len(results[0].acc_list_time), 15))\n",
    "    learning_rate = results[0].lr_model.learning_rate\n",
    "    \n",
    "    [plt.plot(it_row, results[i].acc_list_it) for i in range(len(results))]\n",
    "    plt.legend(batch_sizes, bbox_to_anchor=(1.04, 1))\n",
    "    plt.grid()\n",
    "    plt.title(f\"Change in validation accuracy over iterations by batch size (lr = {learning_rate})\")\n",
    "    plt.savefig('./' + lr_val + '_acc_iter_batch')\n",
    "\n",
    "    \n",
    "    [plt.plot(time_row, results[i].acc_list_time) for i in range(len(results))]\n",
    "    plt.legend(batch_sizes, bbox_to_anchor=(1.04, 1))\n",
    "    plt.grid()\n",
    "    plt.title(f\"Speed of accuracy convergence by batch size (lr = {learning_rate})\")\n",
    "    plt.savefig('./figures/' + lr_val + '_acc_speed_batch')\n",
    "\n",
    "\n",
    "    [plt.plot(it_row, results[i].grad_list_it) for i in range(len(results))]\n",
    "    plt.legend(batch_sizes, bbox_to_anchor=(1.04, 1))\n",
    "    plt.grid()\n",
    "    plt.title(f\"Change in gradient over iterations by batch size (lr = {learning_rate})\")\n",
    "    plt.savefig('./figures/' + lr_val + '_grad_iter_batch')\n",
    "\n",
    "    \n",
    "    [plt.plot(time_row, results[i].grad_list_time) for i in range(len(results))]\n",
    "    plt.legend(batch_sizes, bbox_to_anchor=(1.04, 1))\n",
    "    plt.grid()\n",
    "    plt.title(f\"Speed of gradient convergence by batch size (lr = {learning_rate})\")\n",
    "    plt.savefig('./figures/' + lr_val + '_grad_speed_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (by epochs) with all batch sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with 10000 epochs\n",
    "results = [ get_training_results_batch(1e-4, 3*(1e6), 1e4, batch_size, 10000, 0) for batch_size in batch_sizes]\n",
    "print(\"10000 epochs training results\")\n",
    "for result in results:\n",
    "    test_yh = (result.lr_model.predict(test_X) > 0.5).astype('int')\n",
    "    print(\"Batch size: \", result.batch_size, \" Accuracy: \", accuracy_score(test_y, test_yh))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (by iterations) with all batch sizes and 3 learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_max_iter = 3*(1e6)\n",
    "new_itv = 1e4\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256, 512]\n",
    "results_lr_best = Parallel(n_jobs=-1, verbose=10)(delayed(get_training_results_batch)(1e-4, new_max_iter, new_itv, batch_size, max_epochs=3) for batch_size in batch_sizes)\n",
    "results_lr_low = Parallel(n_jobs=-1, verbose=10)(delayed(get_training_results_batch)(1e-7, new_max_iter, new_itv, batch_size) for batch_size in batch_sizes)\n",
    "results_lr_high = Parallel(n_jobs=-1, verbose=10)(delayed(get_training_results_batch)(0.2, new_max_iter, new_itv, batch_size) for batch_size in batch_sizes)\n",
    "\n",
    "modified_results_lr_best = same_length_lsts(results_lr_best)\n",
    "modified_results_lr_low = same_length_lsts(results_lr_low)\n",
    "modified_results_lr_high = same_length_lsts(results_lr_high)\n",
    "\n",
    "plot_results(modified_results_lr_low, 'lr_low')\n",
    "plot_results(modified_results_lr_best, 'lr_best')\n",
    "plot_results(modified_results_lr_high, 'lr_high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PZaVdLkhHyk"
   },
   "source": [
    "#### 3. Momentum\n",
    "Add momentum to the gradient descent implementation. Trying multiple values for the momentum coefficient, how does it compare to regular gradient descent ? Specifically, analyze the impact of momentum on the conver- gence speed and the quality of the final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFmW6N9KhLi-"
   },
   "outputs": [],
   "source": [
    "# Momentum added to LogisticRegression.py fit_for_vis_complex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq6kKVavhMiR"
   },
   "source": [
    "#### 4.\n",
    "repeat the previous step for a) the smallest batch size and b) largest batch size you tried in 2). In which setting (small mini-batch, large mini-batch, fully batched) is it the most / least effective ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "spNlFyEghPH1"
   },
   "outputs": [],
   "source": [
    "# do"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOe4nJa2XgrVd41GJl4nF9x",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "P2-Optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
