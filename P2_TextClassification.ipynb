{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2-TextClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMujNh7ZV/1DbJOlU2VlOaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbG4Bt9E_8Vo"
      },
      "source": [
        "## **Part 2: Text Classification (20 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmKddKN_6uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b073611-514d-4d1f-d977-167195a9ef40"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize        \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Constants\n",
        "\n",
        "contractions_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
        "# Regular expression for finding contractions\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "lMaOcm5_B-iT",
        "outputId": "735d1735-f0dc-4234-f459-3981538fc24d"
      },
      "source": [
        "# Define dataset paths\n",
        "fake_new_train_dir = r'fake_news/fake_news_train.csv'\n",
        "fake_news_val_dir = r'fake_news/fake_news_val.csv'\n",
        "fake_news_test_dir = r'fake_news/fake_news_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "fake_news_train = pd.read_csv(fake_new_train_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_val = pd.read_csv(fake_news_val_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_test = pd.read_csv(fake_news_test_dir, engine=\"python\", error_bad_lines=False)\n",
        "\n",
        "fake_news_train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping line 5337: unexpected end of data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian fruit is so important to so many people...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FORT WORTH, Texas — Urú Inc. will hold a confe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With three of the four new carriers, the Niger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let's start with the classic annual dividend r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Following are some of the major events to have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5330</th>\n",
              "      <td>With the return of Game of Thrones also comes ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5331</th>\n",
              "      <td>Waller-Bridge is heading the next installment ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332</th>\n",
              "      <td>Share\\nA number of retailers are offering $50,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5333</th>\n",
              "      <td>TEHRAN — Around 4,000 couples attended 34 such...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5334</th>\n",
              "      <td>MANCHESTER UNITED striker Romelu Lukaku would ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5335 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "0     Indian fruit is so important to so many people...      0\n",
              "1     FORT WORTH, Texas — Urú Inc. will hold a confe...      0\n",
              "2     With three of the four new carriers, the Niger...      0\n",
              "3     Let's start with the classic annual dividend r...      0\n",
              "4     Following are some of the major events to have...      1\n",
              "...                                                 ...    ...\n",
              "5330  With the return of Game of Thrones also comes ...      1\n",
              "5331  Waller-Bridge is heading the next installment ...      0\n",
              "5332  Share\\nA number of retailers are offering $50,...      0\n",
              "5333  TEHRAN — Around 4,000 couples attended 34 such...      0\n",
              "5334  MANCHESTER UNITED striker Romelu Lukaku would ...      1\n",
              "\n",
              "[5335 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJUff6sCIxA"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8JYBS6DAuu"
      },
      "source": [
        "# Stemmer function\n",
        "def apply_stemmer(text):\n",
        "    stemmer= PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# Expanding contractions\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Remove stop-words\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
        "        \n",
        "# Preprocessing function\n",
        "def preprocess(df):\n",
        "\n",
        "    # Lowercase everything\n",
        "    df['text'] = df['text'].str.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    # Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "    df['text'] = df['text'].apply(lambda x : expand_contractions(x))\n",
        "\n",
        "    # Remove ponctuation\n",
        "    df['text'] = df['text'].replace(r'[^\\w\\s]', r'', regex=True)\n",
        "\n",
        "    # Remove numbers\n",
        "    df['text'] = df['text'].replace(r'\\d', r'', regex=True)\n",
        "\n",
        "    # Remove special characters (eg: \\n)\n",
        "    df['text'] = df['text'].replace(r'\\\\[a-z]', r'', regex=True)\n",
        "\n",
        "    # Remove stop words\n",
        "    df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
        "\n",
        "    # Stemming\n",
        "    # df[\"text\"] = df[\"text\"].apply(lambda x: apply_stemmer(x))\n",
        "\n",
        "    # Trim whitespaces\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Word level count vectorization\n",
        "    vect = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', token_pattern=r'\\w{1,}', min_df=5, max_df=0.5, stop_words='english', max_features=5000, binary=True)\n",
        "    X_train_counts = vect.fit_transform(df.text)\n",
        "    joblib.dump(vect, \"vectorizer.pkl\")\n",
        "    print(\"Vectorizer vocabulary:\", vect.vocabulary_.get(u'algorithm'))\n",
        "    print(\"Count Vectorizer shape:\", X_train_counts.shape)\n",
        "    \n",
        "\n",
        "    # TF-IDF representation using bag-of-words matrix\n",
        "    tfidf_transform = TfidfTransformer()\n",
        "    X_train_tfidf = tfidf_transform.fit_transform(X_train_counts)\n",
        "    joblib.dump(tfidf_transform, \"tfidf_transform.pkl\")\n",
        "    print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return X_train_tfidf, df\n",
        "\n",
        "# fake_news_train_preprocessed_tfidf, fake_news_train_preprocessed = preprocess(fake_news_train)\n",
        "# fake_news_train_preprocessed"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WotyYfhMsby"
      },
      "source": [
        "# fake_news_train_preprocessed.iloc[0]['text']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPDM3sMp7k_"
      },
      "source": [
        "#### Model Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJWhGdHpjuX"
      },
      "source": [
        "def train_model(x_train, y_train):\n",
        "    # preprocess x_train\n",
        "    x_train_preprocessed_tfidf, x_train_preprocessed  = preprocess(x_train)\n",
        "\n",
        "    # fit model\n",
        "    model = LogisticRegression(C=5, fit_intercept=False, solver='liblinear', multi_class='ovr').fit(x_train_preprocessed_tfidf, y_train)\n",
        "    training_score = model.score(x_train_preprocessed_tfidf, y_train)\n",
        "    joblib.dump(train_model, \"model.pkl\")\n",
        "\n",
        "    print(\"Training score:\", training_score)\n",
        "\n",
        "    return model\n",
        "\n",
        "def transform_preprocess(x_test):\n",
        "    # Load saved pickles\n",
        "    loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "    loaded_tfidf_transform = joblib.load(\"tfidf_transform.pkl\")\n",
        "    loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "    # Transform\n",
        "    x_val_vec = loaded_vectorizer.transform(x_test)\n",
        "    x_val_tfidf = loaded_tfidf_transform.transform(x_val_vec)\n",
        "\n",
        "    return x_val_tfidf\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXH_LHDasBQZ",
        "outputId": "ed2b661b-7c65-4e80-9264-2e1d7d8328e2"
      },
      "source": [
        "# Train model\n",
        "trained_model = train_model(fake_news_train, fake_news_train['label'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: 149\n",
            "Count Vectorizer shape: (5335, 5000)\n",
            "TF-IDF shape: (5335, 5000)\n",
            "\n",
            "\n",
            "Training score: 0.9439550140581069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxGFYrhyt_0N",
        "outputId": "82354592-2a67-437b-9868-dae39d619826"
      },
      "source": [
        "# Test set\n",
        "x_test_tfidf = transform_preprocess(fake_news_test['text'])\n",
        "y_predictions = trained_model.predict(x_test_tfidf)\n",
        "\n",
        "acc_score = accuracy_score(fake_news_test['label'], y_predictions)\n",
        "print(\"Accuracy on test set:\", acc_score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.6426666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpoMyghbtjHf"
      },
      "source": [
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApfBKB7exSqa"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}