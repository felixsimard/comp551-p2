{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2-TextClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/h6EsvO+QyG8Tn2XaI28i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbG4Bt9E_8Vo"
      },
      "source": [
        "## **Part 2: Text Classification (20 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmKddKN_6uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5741fd1d-ec39-4c73-cd45-f18021f83916"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize        \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Constants\n",
        "\n",
        "# contractions_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
        "contractions_dict = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "# Regular expression for finding contractions\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "lMaOcm5_B-iT",
        "outputId": "9848e9dd-f28d-44cd-9124-1f69dca3554b"
      },
      "source": [
        "# Define dataset paths\n",
        "fake_new_train_dir = r'fake_news/fake_news_train.csv'\n",
        "fake_news_val_dir = r'fake_news/fake_news_val.csv'\n",
        "fake_news_test_dir = r'fake_news/fake_news_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "fake_news_train = pd.read_csv(fake_new_train_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_val = pd.read_csv(fake_news_val_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_test = pd.read_csv(fake_news_test_dir, engine=\"python\", error_bad_lines=False)\n",
        "\n",
        "fake_news_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian fruit is so important to so many people...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FORT WORTH, Texas — Urú Inc. will hold a confe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With three of the four new carriers, the Niger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let's start with the classic annual dividend r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Following are some of the major events to have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Warning: small, petty spoilers for the Game of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>Shilpa Shetty will soon make her Bollywood deb...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Add a digital black hole image to the Allstate...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>Share\\nThe name W. L. Gore &amp; Associates might ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>A comprehensive report recapping developments ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "0      Indian fruit is so important to so many people...      0\n",
              "1      FORT WORTH, Texas — Urú Inc. will hold a confe...      0\n",
              "2      With three of the four new carriers, the Niger...      0\n",
              "3      Let's start with the classic annual dividend r...      0\n",
              "4      Following are some of the major events to have...      1\n",
              "...                                                  ...    ...\n",
              "19995  Warning: small, petty spoilers for the Game of...      1\n",
              "19996  Shilpa Shetty will soon make her Bollywood deb...      0\n",
              "19997  Add a digital black hole image to the Allstate...      0\n",
              "19998  Share\\nThe name W. L. Gore & Associates might ...      1\n",
              "19999  A comprehensive report recapping developments ...      1\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJUff6sCIxA"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8JYBS6DAuu"
      },
      "source": [
        "# Stemmer function\n",
        "def apply_stemmer(text):\n",
        "    stemmer= PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# Expanding contractions\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Remove stop-words\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
        "        \n",
        "# Preprocessing function\n",
        "def preprocess(df):\n",
        "\n",
        "    # Lowercase everything\n",
        "    df['text'] = df['text'].str.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    # Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "    df['text'] = df['text'].apply(lambda x : expand_contractions(x))\n",
        "\n",
        "    # Remove ponctuation\n",
        "    df['text'] = df['text'].replace(r'[^\\w\\s]', r'', regex=True)\n",
        "\n",
        "    # Remove numbers\n",
        "    df['text'] = df['text'].replace(r'\\d', r'', regex=True)\n",
        "\n",
        "    # Remove special characters (eg: \\n)\n",
        "    df['text'] = df['text'].replace(r'\\\\[a-z]', r'', regex=True)\n",
        "\n",
        "    # Remove stop words\n",
        "    df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
        "\n",
        "    # Stemming\n",
        "    # df[\"text\"] = df[\"text\"].apply(lambda x: apply_stemmer(x))\n",
        "\n",
        "    # Trim whitespaces\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Word level count vectorization\n",
        "    vect = CountVectorizer(tokenizer=LemmaTokenizer(), analyzer='word', token_pattern=r'\\w{1,}', min_df=5, max_df=0.5, stop_words='english', max_features=5000, binary=True)\n",
        "    X_train_counts = vect.fit_transform(df.text)\n",
        "    joblib.dump(vect, \"vectorizer.pkl\")\n",
        "    print(\"Vectorizer vocabulary:\", vect.vocabulary_.get(u'algorithm'))\n",
        "    print(\"Count Vectorizer shape:\", X_train_counts.shape)\n",
        "    \n",
        "\n",
        "    # TF-IDF representation using bag-of-words matrix\n",
        "    tfidf_transform = TfidfTransformer()\n",
        "    X_train_tfidf = tfidf_transform.fit_transform(X_train_counts)\n",
        "    joblib.dump(tfidf_transform, \"tfidf_transform.pkl\")\n",
        "    print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return X_train_tfidf, df\n",
        "\n",
        "# fake_news_train_preprocessed_tfidf, fake_news_train_preprocessed = preprocess(fake_news_train)\n",
        "# fake_news_train_preprocessed"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WotyYfhMsby"
      },
      "source": [
        "# fake_news_train_preprocessed.iloc[0]['text']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPDM3sMp7k_"
      },
      "source": [
        "#### Model Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJWhGdHpjuX"
      },
      "source": [
        "def train_model(x_train, y_train):\n",
        "    # preprocess x_train\n",
        "    x_train_preprocessed_tfidf, x_train_preprocessed  = preprocess(x_train)\n",
        "\n",
        "    # fit model\n",
        "    model = LogisticRegression(C=5, fit_intercept=False, solver='liblinear', multi_class='ovr').fit(x_train_preprocessed_tfidf, y_train)\n",
        "    training_score = model.score(x_train_preprocessed_tfidf, y_train)\n",
        "    joblib.dump(train_model, \"model.pkl\")\n",
        "\n",
        "    print(\"Training score:\", training_score)\n",
        "\n",
        "    return model\n",
        "\n",
        "def transform_preprocess(x_test):\n",
        "    # Load saved pickles\n",
        "    loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "    loaded_tfidf_transform = joblib.load(\"tfidf_transform.pkl\")\n",
        "    loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "    # Transform\n",
        "    x_val_vec = loaded_vectorizer.transform(x_test)\n",
        "    x_val_tfidf = loaded_tfidf_transform.transform(x_val_vec)\n",
        "\n",
        "    return x_val_tfidf\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXH_LHDasBQZ",
        "outputId": "9f3bdcf5-b35a-46ba-bf2e-66ad97692444"
      },
      "source": [
        "# Train model\n",
        "trained_model = train_model(fake_news_train, fake_news_train['label'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: None\n",
            "Count Vectorizer shape: (20000, 5000)\n",
            "TF-IDF shape: (20000, 5000)\n",
            "\n",
            "\n",
            "Training score: 0.8302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxGFYrhyt_0N",
        "outputId": "38b98c44-5dfc-41bd-c9a7-e64e7f7994c4"
      },
      "source": [
        "\n",
        "# Test set\n",
        "x_test_tfidf = transform_preprocess(fake_news_test['text'])\n",
        "y_predictions = trained_model.predict(x_test_tfidf)\n",
        "\n",
        "acc_score = accuracy_score(fake_news_test['label'], y_predictions)\n",
        "print(\"Accuracy on test set:\", acc_score)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpoMyghbtjHf"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApfBKB7exSqa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}