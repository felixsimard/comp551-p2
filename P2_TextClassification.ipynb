{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2-TextClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOi8P4Srn1W7l/rpAH6OUj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbG4Bt9E_8Vo"
      },
      "source": [
        "## **Part 2: Text Classification (20 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmKddKN_6uG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import joblib\n",
        "\n",
        "# Constants"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "lMaOcm5_B-iT",
        "outputId": "b6c93a7f-cc50-4c9c-eefa-91d1e39909b3"
      },
      "source": [
        "# Define dataset paths\n",
        "fake_new_train_dir = r'fake_news/fake_news_train.csv'\n",
        "fake_news_val_dir = r'fake_news/fake_news_val.csv'\n",
        "fake_news_test_dir = r'fake_news/fake_news_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "fake_news_train = pd.read_csv(fake_new_train_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_val = pd.read_csv(fake_news_val_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_test = pd.read_csv(fake_news_test_dir, engine=\"python\", error_bad_lines=False)\n",
        "\n",
        "fake_news_train"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping line 3994: unexpected end of data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian fruit is so important to so many people...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FORT WORTH, Texas — Urú Inc. will hold a confe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With three of the four new carriers, the Niger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let's start with the classic annual dividend r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Following are some of the major events to have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3987</th>\n",
              "      <td>sport, local-sport,\\nMelbourne Victory is stre...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3988</th>\n",
              "      <td>LIONEL MESSI is still struggling after being h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3989</th>\n",
              "      <td>FRANKFURT, Finland — Finland’s economy has gro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3990</th>\n",
              "      <td>Vermont health officials say two coyotes have ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3991</th>\n",
              "      <td>SIR Jake Owen was something of a stranger when...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3992 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "0     Indian fruit is so important to so many people...      0\n",
              "1     FORT WORTH, Texas — Urú Inc. will hold a confe...      0\n",
              "2     With three of the four new carriers, the Niger...      0\n",
              "3     Let's start with the classic annual dividend r...      0\n",
              "4     Following are some of the major events to have...      1\n",
              "...                                                 ...    ...\n",
              "3987  sport, local-sport,\\nMelbourne Victory is stre...      1\n",
              "3988  LIONEL MESSI is still struggling after being h...      1\n",
              "3989  FRANKFURT, Finland — Finland’s economy has gro...      0\n",
              "3990  Vermont health officials say two coyotes have ...      1\n",
              "3991  SIR Jake Owen was something of a stranger when...      0\n",
              "\n",
              "[3992 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJUff6sCIxA"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8JYBS6DAuu"
      },
      "source": [
        "# Make a preprocessing function\n",
        "def preprocess(df):\n",
        "\n",
        "    # Lowercase everything\n",
        "    df['text'] = df['text'].str.lower()\n",
        "\n",
        "    # Remove ponctuation\n",
        "    df['text'] = df['text'].replace(r'[^\\w\\s]', r'', regex=True)\n",
        "\n",
        "    # Trim whitespaces\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Bag of words + Tokenization\n",
        "    vect = CountVectorizer()\n",
        "    X_train_counts = vect.fit_transform(df.text)\n",
        "    joblib.dump(vect, \"vectorizer.pkl\")\n",
        "    print(\"Vectorizer vocabulary:\", vect.vocabulary_.get(u'algorithm'))\n",
        "    print(\"Count Vectorizer shape:\", X_train_counts.shape)\n",
        "    \n",
        "\n",
        "    # TF-IDF repsenation using bag-of-words matrix\n",
        "    tfidf_transform = TfidfTransformer()\n",
        "    X_train_tfidf = tfidf_transform.fit_transform(X_train_counts)\n",
        "    joblib.dump(tfidf_transform, \"tfidf_transform.pkl\")\n",
        "    print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return X_train_tfidf\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPDM3sMp7k_"
      },
      "source": [
        "#### Model Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJWhGdHpjuX"
      },
      "source": [
        "def train_model(x_train, y_train):\n",
        "    # preprocess x_train\n",
        "    x_train_preprocessed = preprocess(x_train)\n",
        "\n",
        "    # fit model\n",
        "    model = LogisticRegression().fit(x_train_preprocessed, y_train)\n",
        "    training_score = model.score(x_train_preprocessed, y_train)\n",
        "    joblib.dump(train_model, \"model.pkl\")\n",
        "\n",
        "    print(\"Training score:\", score)\n",
        "\n",
        "    return model\n",
        "\n",
        "def transform_preprocess(x_test):\n",
        "    # Load saved pickles\n",
        "    loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "    loaded_tfidf_transform = joblib.load(\"tfidf_transform.pkl\")\n",
        "    loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "    # Transform\n",
        "    x_val_vec = loaded_vectorizer.transform(x_test)\n",
        "    x_val_tfidf = loaded_tfidf_transform.transform(x_val_vec)\n",
        "\n",
        "    return x_val_tfidf\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXH_LHDasBQZ",
        "outputId": "6883f294-684e-42af-e207-56c45d3ade7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train model\n",
        "trained_model = train_model(fake_news_train, fake_news_train['label'])\n",
        "\n",
        "# Test set\n",
        "x_test_tfidf = transform_preprocess(fake_news_test['text'])\n",
        "y_predictions = trained_model.predict(x_test_tfidf)\n",
        "\n",
        "acc_score = accuracy_score(fake_news_test['label'], y_predictions)\n",
        "print(\"Accuracy on test set:\", acc_score)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: 6941\n",
            "Count Vectorizer shape: (3992, 79057)\n",
            "TF-IDF shape: (3992, 79057)\n",
            "\n",
            "\n",
            "Training score: 0.8469438877755511\n",
            "Accuracy on test set: 0.6316666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpoMyghbtjHf"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    }
  ]
}