{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2-TextClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpf8glUeVVjvJfVdqsWFr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbG4Bt9E_8Vo"
      },
      "source": [
        "## **Part 2: Text Classification (20 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WmKddKN_6uG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea61c85-5de6-4dc6-ab4f-0dd377276b29"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Constants\n",
        "\n",
        "contractions_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
        "# Regular expression for finding contractions\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "lMaOcm5_B-iT",
        "outputId": "23aabe65-f41c-4ee0-b350-229c8647c518"
      },
      "source": [
        "# Define dataset paths\n",
        "fake_new_train_dir = r'fake_news/fake_news_train.csv'\n",
        "fake_news_val_dir = r'fake_news/fake_news_val.csv'\n",
        "fake_news_test_dir = r'fake_news/fake_news_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "fake_news_train = pd.read_csv(fake_new_train_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_val = pd.read_csv(fake_news_val_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_test = pd.read_csv(fake_news_test_dir, engine=\"python\", error_bad_lines=False)\n",
        "\n",
        "fake_news_train"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian fruit is so important to so many people...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FORT WORTH, Texas — Urú Inc. will hold a confe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With three of the four new carriers, the Niger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let's start with the classic annual dividend r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Following are some of the major events to have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>Warning: small, petty spoilers for the Game of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>Shilpa Shetty will soon make her Bollywood deb...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>Add a digital black hole image to the Allstate...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>Share\\nThe name W. L. Gore &amp; Associates might ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>A comprehensive report recapping developments ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "0      Indian fruit is so important to so many people...      0\n",
              "1      FORT WORTH, Texas — Urú Inc. will hold a confe...      0\n",
              "2      With three of the four new carriers, the Niger...      0\n",
              "3      Let's start with the classic annual dividend r...      0\n",
              "4      Following are some of the major events to have...      1\n",
              "...                                                  ...    ...\n",
              "19995  Warning: small, petty spoilers for the Game of...      1\n",
              "19996  Shilpa Shetty will soon make her Bollywood deb...      0\n",
              "19997  Add a digital black hole image to the Allstate...      0\n",
              "19998  Share\\nThe name W. L. Gore & Associates might ...      1\n",
              "19999  A comprehensive report recapping developments ...      1\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJUff6sCIxA"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8JYBS6DAuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "e1cd875d-e3bd-4e76-c52d-4860f9f89c23"
      },
      "source": [
        "# Stemmer function\n",
        "def apply_stemmer(text):\n",
        "    stemmer= PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# Expanding contractions\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Remove stop-words\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(df):\n",
        "\n",
        "    # Lowercase everything\n",
        "    df['text'] = df['text'].str.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    # Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "    df['text'] = df['text'].apply(lambda x : expand_contractions(x))\n",
        "\n",
        "    # Remove ponctuation\n",
        "    df['text'] = df['text'].replace(r'[^\\w\\s]', r'', regex=True)\n",
        "\n",
        "    # Remove numbers\n",
        "    df['text'] = df['text'].replace(r'\\d', r'', regex=True)\n",
        "\n",
        "    # Remove special characters (eg: \\n)\n",
        "    df['text'] = df['text'].replace(r'\\\\[a-z]', r'', regex=True)\n",
        "\n",
        "    # Remove stop words\n",
        "    df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
        "\n",
        "    # Stemming\n",
        "    df[\"text\"] = df[\"text\"].apply(lambda x: apply_stemmer(x))\n",
        "\n",
        "    # Trim whitespaces\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Bag of words + Tokenization\n",
        "    vect = CountVectorizer()\n",
        "    X_train_counts = vect.fit_transform(df.text)\n",
        "    joblib.dump(vect, \"vectorizer.pkl\")\n",
        "    print(\"Vectorizer vocabulary:\", vect.vocabulary_.get(u'algorithm'))\n",
        "    print(\"Count Vectorizer shape:\", X_train_counts.shape)\n",
        "    \n",
        "\n",
        "    # TF-IDF representation using bag-of-words matrix\n",
        "    tfidf_transform = TfidfTransformer()\n",
        "    X_train_tfidf = tfidf_transform.fit_transform(X_train_counts)\n",
        "    joblib.dump(tfidf_transform, \"tfidf_transform.pkl\")\n",
        "    print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return X_train_tfidf, df\n",
        "\n",
        "fake_news_train_preprocessed_tfidf, fake_news_train_preprocessed = preprocess(fake_news_train)\n",
        "fake_news_train_preprocessed"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: 3155\n",
            "Count Vectorizer shape: (20000, 138086)\n",
            "TF-IDF shape: (20000, 138086)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>indian fruit import mani peopl season come end...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fort worth texa urú inc hold confer call today...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>three four new carrier nigerian airspac manag ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>let start classic annual dividend reinvest pla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>follow major event occur apr reuter britain fr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>warn small petti spoiler game throne season ei...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>shilpa shetti soon make bollywood debut opposi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>add digit black hole imag allstat compani year...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>share name w l gore associ might immedi ring b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>comprehens report recap develop pakistani cybe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "0      indian fruit import mani peopl season come end...      0\n",
              "1      fort worth texa urú inc hold confer call today...      0\n",
              "2      three four new carrier nigerian airspac manag ...      0\n",
              "3      let start classic annual dividend reinvest pla...      0\n",
              "4      follow major event occur apr reuter britain fr...      1\n",
              "...                                                  ...    ...\n",
              "19995  warn small petti spoiler game throne season ei...      1\n",
              "19996  shilpa shetti soon make bollywood debut opposi...      0\n",
              "19997  add digit black hole imag allstat compani year...      0\n",
              "19998  share name w l gore associ might immedi ring b...      1\n",
              "19999  comprehens report recap develop pakistani cybe...      1\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "9WotyYfhMsby",
        "outputId": "40c2d80a-9f9b-44bf-fee8-612ccc236afa"
      },
      "source": [
        "fake_news_train_preprocessed.iloc[0]['text']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'indian fruit import mani peopl season come end first stop pick someth fresh tangi sugar chop papaya appl littl complic good fat come sugar well milk almond two great sourc fat well coupl fruit avocado okra result healthi delici appletoberri spread easi yummi delici oz appl appleliv bon bon tbsp sugar tsp almond extract freshli ground black pepper tbsp rice vinegar thick lightflavor bon bon season salt pepper tast vari thick finish add fruit bowl small mixer fit larg paddl attach line bowl nonstick cook spray use spatula meal add enough sugar almond extract shake excess mixtur thick solid remov bowl mixer slot spoon mix work spatula ballerina fold sugar need combin egg mixtur rice vinegar almond extract stir berri stirfri combin minut done serv nutrit inform per serv calori g fat g satur fat mg cholesterol g carbohydr g sugar g protein mg sodium g fiber view oil discov local salad dress instruct place shred bon bon food processor process smooth use sharp knife remov chunk place bowl fruit bowl grind minut small bowl combin appl fat oil drain meanwhil cut fruit wedg garnish lemon slice serv recip courtesi pieflix major health food chain see recip go pieflixcom receiv email alert canadian food stori kitchen gadget'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPDM3sMp7k_"
      },
      "source": [
        "#### Model Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJWhGdHpjuX"
      },
      "source": [
        "def train_model(x_train, y_train):\n",
        "    # preprocess x_train\n",
        "    x_train_preprocessed_tfidf, x_train_preprocessed  = preprocess(x_train)\n",
        "\n",
        "    # fit model\n",
        "    model = LogisticRegression().fit(x_train_preprocessed_tfidf, y_train)\n",
        "    training_score = model.score(x_train_preprocessed_tfidf, y_train)\n",
        "    joblib.dump(train_model, \"model.pkl\")\n",
        "\n",
        "    print(\"Training score:\", training_score)\n",
        "\n",
        "    return model\n",
        "\n",
        "def transform_preprocess(x_test):\n",
        "    # Load saved pickles\n",
        "    loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "    loaded_tfidf_transform = joblib.load(\"tfidf_transform.pkl\")\n",
        "    loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "    # Transform\n",
        "    x_val_vec = loaded_vectorizer.transform(x_test)\n",
        "    x_val_tfidf = loaded_tfidf_transform.transform(x_val_vec)\n",
        "\n",
        "    return x_val_tfidf\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXH_LHDasBQZ",
        "outputId": "5cebb663-8da9-4820-88ba-94df834a8675"
      },
      "source": [
        "# Train model\n",
        "trained_model = train_model(fake_news_train, fake_news_train['label'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: 3118\n",
            "Count Vectorizer shape: (20000, 136944)\n",
            "TF-IDF shape: (20000, 136944)\n",
            "\n",
            "\n",
            "Training score: 0.84045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxGFYrhyt_0N",
        "outputId": "ed0064d4-663c-4c6a-ae44-839ca6fcb6d3"
      },
      "source": [
        "# Test set\n",
        "x_test_tfidf = transform_preprocess(fake_news_test['text'])\n",
        "y_predictions = trained_model.predict(x_test_tfidf)\n",
        "\n",
        "acc_score = accuracy_score(fake_news_test['label'], y_predictions)\n",
        "print(\"Accuracy on test set:\", acc_score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpoMyghbtjHf"
      },
      "source": [
        "\n",
        "    \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApfBKB7exSqa"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}