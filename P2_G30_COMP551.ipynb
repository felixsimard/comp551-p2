{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P2_G30_COMP551.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEQZpNmMHz2WDYdPW/9Pvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p2/blob/main/P2_G30_COMP551.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKWQZL1Fckh"
      },
      "source": [
        "\n",
        "# **COMP 551 - Applied Machine Learning**\n",
        "\n",
        "### MiniProject 2: Optimization and Text Classification\n",
        "\n",
        "Takuya Ishii () <br>\n",
        "Felix Simard (260865674) <br>\n",
        "Tyler Watson ()\n",
        "\n",
        "**Group 30** <br>\n",
        "Oct 21th, 2021\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLAmMFSFuWx"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okyyCyUiFWUM",
        "outputId": "3408e920-2a3e-40ec-9b9d-45d2a813a4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize        \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnS6qo5F73z"
      },
      "source": [
        "## **Part 1: Optimization**\n",
        "Link to datasets: https://github.com/felixsimard/comp551-p2/tree/main/diabetes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLb3jVqHF60C"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypaRT_8fGFZM"
      },
      "source": [
        "## **Part 2: Text Classification**\n",
        "Link to datasets: https://github.com/felixsimard/comp551-p2/tree/main/fake_news\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhJa_mURGM2z"
      },
      "source": [
        "# Define constants\n",
        "\n",
        "# Contractions dictonary\n",
        "# Reference: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions_dict = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "# Regular expression for finding contractions\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('subject')\n",
        "stop_words.add('http')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO_aRWRonY_m",
        "outputId": "e5782dbf-fa62-49ac-9c05-fb635a4a19f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# Define dataset paths\n",
        "fake_new_train_dir = r'fake_news/fake_news_train.csv'\n",
        "fake_news_val_dir = r'fake_news/fake_news_val.csv'\n",
        "fake_news_test_dir = r'fake_news/fake_news_test.csv'\n",
        "\n",
        "# Load datasets\n",
        "fake_news_train = pd.read_csv(fake_new_train_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_val = pd.read_csv(fake_news_val_dir, engine=\"python\", error_bad_lines=False)\n",
        "fake_news_test = pd.read_csv(fake_news_test_dir, engine=\"python\", error_bad_lines=False)\n",
        "\n",
        "fake_news_train"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping line 5985: unexpected end of data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian fruit is so important to so many people...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FORT WORTH, Texas — Urú Inc. will hold a confe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>With three of the four new carriers, the Niger...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Let's start with the classic annual dividend r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Following are some of the major events to have...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5978</th>\n",
              "      <td>White House press secretary Sarah Huckabee San...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5979</th>\n",
              "      <td>Organizers of the Plantation Youth Fishing Cli...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5980</th>\n",
              "      <td>Share\\nMicrosoft this past weekend quietly upd...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>NDP Leader Rachel Notley says her party’s sign...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5982</th>\n",
              "      <td>Despite election triumphs, the youth that over...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5983 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "0     Indian fruit is so important to so many people...      0\n",
              "1     FORT WORTH, Texas — Urú Inc. will hold a confe...      0\n",
              "2     With three of the four new carriers, the Niger...      0\n",
              "3     Let's start with the classic annual dividend r...      0\n",
              "4     Following are some of the major events to have...      1\n",
              "...                                                 ...    ...\n",
              "5978  White House press secretary Sarah Huckabee San...      1\n",
              "5979  Organizers of the Plantation Youth Fishing Cli...      0\n",
              "5980  Share\\nMicrosoft this past weekend quietly upd...      1\n",
              "5981  NDP Leader Rachel Notley says her party’s sign...      1\n",
              "5982  Despite election triumphs, the youth that over...      0\n",
              "\n",
              "[5983 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXJPmBjn0T2"
      },
      "source": [
        "#### Quick dataset scan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKu1hg6n4hU",
        "outputId": "0550601c-a5e0-4ce7-d82f-0a301302dc97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Look at label value distribution and save ratios for Logitic Regression class weights\n",
        "train_value_counts = fake_news_train.label.value_counts()\n",
        "total = train_value_counts[0] + train_value_counts[1]\n",
        "train_labels_ratio = {\n",
        "    0: train_value_counts[0] / total,\n",
        "    1: train_value_counts[1] / total,\n",
        "}\n",
        "\n",
        "train_value_counts.plot(kind='bar')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f76145a7490>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwVJu5Ln5qi"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuwJn_tqn7VT"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrSBM45Ln9LL"
      },
      "source": [
        "# Stemmer function\n",
        "def apply_stemmer(text):\n",
        "    stemmer= PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "# Expanding contractions\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Remove stop-words\n",
        "# Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
        "        \n",
        "# Preprocessing function\n",
        "def preprocess(df):\n",
        "\n",
        "    # Lowercase everything\n",
        "    df['text'] = df['text'].str.lower()\n",
        "\n",
        "    # Expand contractions\n",
        "    # Reference: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/\n",
        "    df['text'] = df['text'].apply(lambda x : expand_contractions(x))\n",
        "\n",
        "    # Remove ponctuation\n",
        "    df['text'] = df['text'].replace(r'[^\\w\\s]', r'', regex=True)\n",
        "\n",
        "    # Remove numbers\n",
        "    df['text'] = df['text'].replace(r'\\d', r'', regex=True)\n",
        "\n",
        "    # Remove special characters (eg: \\n)\n",
        "    df['text'] = df['text'].replace(r'\\\\[a-z]', r'', regex=True)\n",
        "\n",
        "    # Remove stop words\n",
        "    df['text'] = df['text'].apply(lambda x : remove_stopwords(x))\n",
        "\n",
        "    # Stemming - not helping generalization...\n",
        "    # df[\"text\"] = df[\"text\"].apply(lambda x: apply_stemmer(x))\n",
        "\n",
        "    # Trim whitespaces\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Word level count vectorization\n",
        "    vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', min_df=5, max_df=0.5, max_features=20000, binary=True)\n",
        "    X_train_counts = vect.fit_transform(df.text)\n",
        "    joblib.dump(vect, \"vectorizer.pkl\")\n",
        "    print(\"Vectorizer vocabulary:\", vect.vocabulary_.get(u'algorithm'))\n",
        "    print(\"Count Vectorizer shape:\", X_train_counts.shape)\n",
        "    \n",
        "\n",
        "    # TF-IDF representation using bag-of-words matrix\n",
        "    tfidf_transform = TfidfTransformer()\n",
        "    X_train_tfidf = tfidf_transform.fit_transform(X_train_counts)\n",
        "    joblib.dump(tfidf_transform, \"tfidf_transform.pkl\")\n",
        "    print(\"TF-IDF shape:\", X_train_tfidf.shape)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return X_train_tfidf, df\n",
        "\n",
        "# fake_news_train_preprocessed_tfidf, fake_news_train_preprocessed = preprocess(fake_news_train)\n",
        "# fake_news_train_preprocessed"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_n2ajNoC0O"
      },
      "source": [
        "#### Model Training, Validation, Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rypA_f2LoV7x"
      },
      "source": [
        "def train_model(x_train, y_train, params={}):\n",
        "    # preprocess x_train\n",
        "    x_train_preprocessed_tfidf, x_train_preprocessed  = preprocess(x_train)\n",
        "\n",
        "    # fit model\n",
        "    model = LogisticRegression(C=1, class_weight='balanced', fit_intercept=False, tol=0.001, solver='liblinear', multi_class='ovr')\n",
        "    trained = model.fit(x_train_preprocessed_tfidf, y_train)\n",
        "    training_score = trained.score(x_train_preprocessed_tfidf, y_train)\n",
        "    joblib.dump(trained, \"model.pkl\")\n",
        "\n",
        "    print(\"Training score:\", training_score)\n",
        "\n",
        "    # Grid cross validation\n",
        "    grid = GridSearchCV(estimator=model, param_grid=params)\n",
        "    grid.fit(x_train_preprocessed_tfidf, y_train)\n",
        "    print(grid)\n",
        "    print(grid.best_score_)\n",
        "    print(grid.best_estimator_)\n",
        "\n",
        "    return trained\n",
        "\n",
        "def transform_preprocess(x_test):\n",
        "    # Load saved pickles\n",
        "    loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
        "    loaded_tfidf_transform = joblib.load(\"tfidf_transform.pkl\")\n",
        "    loaded_model = joblib.load(\"model.pkl\")\n",
        "\n",
        "    # Transform\n",
        "    x_val_vec = loaded_vectorizer.transform(x_test)\n",
        "    x_val_tfidf = loaded_tfidf_transform.transform(x_val_vec)\n",
        "\n",
        "    return x_val_tfidf\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzL2qpeoYnH"
      },
      "source": [
        "# Grid search parameters for cross validation\n",
        "param_grid = [\n",
        "          {'C': [1, 5, 10], 'solver': ['newton-cg', 'liblinear', 'lbfgs'], 'tol': [0.0001, 0.001, 0.01, 0.1], 'max_iter': [100, 200]}\n",
        "]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAmqzquTodgf",
        "outputId": "ff41d386-ea6d-4e15-bdb9-8af2e73ba585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train model\n",
        "trained_model = train_model(fake_news_train, fake_news_train['label'], params=param_grid)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer vocabulary: 537\n",
            "Count Vectorizer shape: (5983, 20000)\n",
            "TF-IDF shape: (5983, 20000)\n",
            "\n",
            "\n",
            "Training score: 0.9421694801938827\n",
            "GridSearchCV(cv=None, error_score=nan,\n",
            "             estimator=LogisticRegression(C=1, class_weight='balanced',\n",
            "                                          dual=False, fit_intercept=False,\n",
            "                                          intercept_scaling=1, l1_ratio=None,\n",
            "                                          max_iter=100, multi_class='ovr',\n",
            "                                          n_jobs=None, penalty='l2',\n",
            "                                          random_state=None, solver='liblinear',\n",
            "                                          tol=0.001, verbose=0,\n",
            "                                          warm_start=False),\n",
            "             iid='deprecated', n_jobs=None,\n",
            "             param_grid=[{'C': [1, 5, 10], 'max_iter': [100, 200],\n",
            "                          'solver': ['newton-cg', 'liblinear', 'lbfgs'],\n",
            "                          'tol': [0.0001, 0.001, 0.01, 0.1]}],\n",
            "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
            "             scoring=None, verbose=0)\n",
            "0.6941303928718117\n",
            "LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
            "                   fit_intercept=False, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='ovr', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.1, verbose=0,\n",
            "                   warm_start=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mat3P-0loer-",
        "outputId": "ea10fbaa-bbe5-4b14-d8f0-a17249c58541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Test set\n",
        "x_test_tfidf = transform_preprocess(fake_news_test['text'])\n",
        "y_predictions = trained_model.predict(x_test_tfidf)\n",
        "\n",
        "acc_score = accuracy_score(fake_news_test['label'], y_predictions)\n",
        "print(\"Accuracy on test set:\", acc_score)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.6883333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqv6cO-5ojAn"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}